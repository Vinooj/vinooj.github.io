<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Feature Selection & Preprocessing Strategies</title>
    <style>
        /* General Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Body Styling - White and Pastel Colors */
        body {
            font-family: 'Inter', sans-serif; /* Using Inter as per instructions */
            background: #f0f4f8; /* Light pastel blue-grey */
            min-height: 100vh;
            padding: 20px;
            color: #333;
        }

        /* Main Container */
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: #ffffff; /* White background */
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.08); /* Softer shadow */
        }

        /* Header Styling */
        .header {
            text-align: center;
            margin-bottom: 40px;
            background: linear-gradient(135deg, #a8dadc, #457b9d); /* Pastel gradient */
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .header h1 {
            font-size: 2.8rem; /* Slightly larger */
            margin-bottom: 10px;
            font-weight: 800; /* Bolder */
        }

        .header p {
            font-size: 1.3rem; /* Slightly larger */
            color: #6a7f8e; /* Darker pastel text */
        }

        /* Tree Structure Container */
        .tree-container {
            background: #fdfdfd; /* Off-white for tree */
            border-radius: 15px;
            padding: 30px;
            margin: 20px 0;
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.05); /* Lighter shadow */
            border: 1px solid #e0e7eb; /* Soft border */
        }

        .tree {
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.6;
            color: #333;
            white-space: pre-wrap;
            overflow-x: auto;
        }

        /* Priority Colors (adjusted to pastel palette) */
        .priority-1 { color: #e63946; font-weight: bold; } /* Strong red for critical */
        .priority-2 { color: #f4a261; font-weight: bold; } /* Orange for high */
        .priority-3 { color: #457b9d; font-weight: bold; } /* Blue for medium-high */
        .priority-4 { color: #8d86c9; font-weight: bold; } /* Purple for medium */
        .priority-5 { color: #6a994e; font-weight: bold; } /* Green for context-dependent */

        /* Package styling */
        .package {
            background: #a8dadc; /* Pastel cyan */
            color: #333; /* Dark text on pastel */
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 500;
            margin: 0 3px;
            display: inline-block;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1); /* Small shadow for depth */
        }

        .method-description {
            font-style: italic;
            color: #777; /* Softer grey */
            margin-left: 20px;
            font-size: 12px;
        }

        /* Importance Marker Styling */
        .importance-marker {
            font-size: 0.8em; /* Smaller font size */
            font-weight: bold;
            color: #457b9d; /* Blue color for the marker */
            background-color: #e0f2f7; /* Light background */
            padding: 2px 6px;
            border-radius: 8px;
            margin-left: 8px;
            vertical-align: middle; /* Align with text */
            box-shadow: 0 1px 3px rgba(0,0,0,0.05);
        }
        .importance-marker.level-5 { background-color: #fcebeb; color: #e63946; } /* Critical */
        .importance-marker.level-4 { background-color: #fff3e0; color: #f4a261; } /* High */
        .importance-marker.level-3 { background-color: #e0f2f7; color: #457b9d; } /* Medium-High */
        .importance-marker.level-2 { background-color: #f1eff8; color: #8d86c9; } /* Medium */
        .importance-marker.level-1 { background-color: #e6f5e6; color: #6a994e; } /* Context-Dependent */

        .importance-legend {
            font-size: 0.9em;
            margin-bottom: 20px;
            padding: 15px;
            background-color: #f8fbfd;
            border-left: 4px solid #457b9d;
            border-radius: 8px;
            color: #555;
        }
        .importance-legend strong {
            color: #333;
        }


        /* Section Header Styling */
        .section-header {
            background: linear-gradient(135deg, #a8dadc, #457b9d); /* Pastel gradient */
            color: white;
            padding: 15px 25px;
            border-radius: 10px;
            margin: 25px 0 15px 0;
            font-size: 1.4rem; /* Slightly larger */
            font-weight: 600;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.15); /* Softer shadow */
        }

        /* Pipeline Order Cards */
        .pipeline-order {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .phase-card {
            background: #ffffff;
            border-radius: 15px;
            padding: 20px;
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.05);
            border-left: 5px solid;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .phase-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 12px 25px rgba(0, 0, 0, 0.1);
        }

        /* Phase Card Border Colors (adjusted to pastel palette) */
        .phase-1 { border-left-color: #e63946; }
        .phase-2 { border-left-color: #f4a261; }
        .phase-3 { border-left-color: #457b9d; }
        .phase-4 { border-left-color: #8d86c9; }
        .phase-5 { border-left-color: #6a994e; }
        .phase-6 { border-left-color: #a2d2ff; } /* New pastel color */
        .phase-7 { border-left-color: #cdb4db; } /* New pastel color */


        .phase-card h3 {
            font-size: 1.25rem;
            margin-bottom: 10px;
            color: #333;
        }

        .phase-card p {
            font-size: 0.95rem;
            line-height: 1.5;
            color: #555;
        }

        /* Package Table Styling */
        .package-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: #ffffff;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.05);
        }

        .package-table th {
            background: #457b9d; /* Darker pastel blue */
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        .package-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #e0e7eb; /* Softer border */
            color: #444;
        }

        .package-table tr:hover {
            background: #f8fbfd; /* Very light hover */
        }

        /* Best Practices Section */
        .best-practices {
            background: linear-gradient(135deg, #a8dadc, #457b9d); /* Pastel gradient */
            color: white;
            border-radius: 15px;
            padding: 25px;
            margin: 30px 0;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
        }

        .best-practices h3 {
            margin-bottom: 15px;
            font-size: 1.6rem; /* Slightly larger */
        }

        .best-practices ol {
            padding-left: 25px; /* More padding */
        }

        .best-practices li {
            margin-bottom: 10px; /* More spacing */
            line-height: 1.6;
            font-size: 1.05rem;
        }

        /* Usage Notes */
        .usage-notes {
            background: #e0f2f7; /* Very light blue */
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border-left: 4px solid #457b9d; /* Matching pastel blue */
            color: #333;
            font-size: 0.95rem;
            line-height: 1.5;
        }

        .code-snippet {
            background: #2c3e50; /* Dark background for code */
            color: #ecf0f1;
            padding: 15px;
            border-radius: 10px;
            font-family: 'Fira Code', 'Courier New', monospace; /* Fira Code for better readability */
            font-size: 13px;
            margin: 10px 0;
            overflow-x: auto;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }

        .highlight {
            background: #ffe6a7; /* Pastel yellow highlight */
            padding: 2px 5px;
            border-radius: 3px;
            font-weight: bold;
            color: #333; /* Dark text on highlight */
        }

        /* Responsive Adjustments */
        @media (max-width: 768px) {
            .container {
                padding: 15px;
            }
            
            .header h1 {
                font-size: 2.2rem;
            }
            
            .header p {
                font-size: 1rem;
            }

            .tree {
                font-size: 11px;
            }
            
            .pipeline-order {
                grid-template-columns: 1fr;
            }

            .section-header {
                font-size: 1.2rem;
                padding: 12px 20px;
            }

            .phase-card {
                padding: 15px;
            }

            .package-table th, .package-table td {
                padding: 10px;
                font-size: 0.9rem;
            }

            .best-practices h3 {
                font-size: 1.4rem;
            }

            .best-practices li {
                font-size: 0.95rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🎯 ML Feature Selection & Preprocessing Strategies</h1>
            <p>Comprehensive Guide with Python Package Specifications</p>
        </div>

        <div class="section-header">
            🌳 Complete Machine Learning Pipeline Tree
        </div>

        <div class="tree-container">
            <div class="importance-legend">
                <strong>Importance Marker Scale:</strong> This scale indicates the general usage and importance among ML practitioners (1 to 5, where 5 is most widely used/critical, and 1 is less common/context-dependent).
            </div>
            <div class="tree">
<span class="priority-1">COMPLETE MACHINE LEARNING PIPELINE</span>
│
├── <span class="priority-1">1. DATA PREPROCESSING & CLEANING (CRITICAL - First Priority)</span> <span class="importance-marker level-5">(5/5)</span>
│   ├── <span class="highlight">1.1 Data Cleaning</span> <span class="importance-marker level-5">(5/5)</span>
│   │   ├── Handling Inconsistent Data <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">pandas.DataFrame.replace()</span>
│   │   │   ├── <span class="package">regex</span>
│   │   │   └── <span class="method-description">✓ Correcting typos, standardizing formats</span>
│   │   ├── Removing Duplicates <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">pandas.DataFrame.drop_duplicates()</span>
│   │   │   └── <span class="method-description">✓ Ensures unique records</span>
│   │   └── Handling Noise <span class="importance-marker level-4">(4/5)</span>
│   │       ├── Binning <span class="importance-marker level-4">(4/5)</span>
│   │       ├── Regression <span class="importance-marker level-3">(3/5)</span>
│   │       └── Clustering <span class="importance-marker level-3">(3/5)</span>
│   │
│   ├── <span class="highlight">1.2 Imputation Strategies (Handling Missing Values)</span> <span class="importance-marker level-5">(5/5)</span>
│   │   ├── Simple Imputation <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── Mean/Median/Mode Imputation <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">sklearn.impute.SimpleImputer</span>
│   │   │   │   ├── <span class="package">pandas.DataFrame.fillna()</span>
│   │   │   │   └── <span class="method-description">✓ Fast, simple baseline for numerical/categorical data</span>
│   │   │   ├── Forward/Backward Fill <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">pandas.DataFrame.fillna(method='ffill')</span>
│   │   │   │   ├── <span class="package">pandas.DataFrame.fillna(method='bfill')</span>
│   │   │   │   └── <span class="method-description">✓ Ideal for time-series data with temporal dependencies</span>
│   │   │   └── Constant Value Imputation <span class="importance-marker level-4">(4/5)</span>
│   │   │       ├── <span class="package">sklearn.impute.SimpleImputer(strategy='constant')</span>
│   │   │       └── <span class="method-description">✓ Domain-specific constants (e.g., 0, 'Unknown', 'Missing')</span>
│   │   ├── Advanced Imputation <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── Iterative Imputation (MICE) <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.impute.IterativeImputer</span>
│   │   │   │   ├── <span class="package">fancyimpute.IterativeImputer</span>
│   │   │   │   └── <span class="method-description">✓ Robust multivariate imputation using chained equations</span>
│   │   │   ├── K-Nearest Neighbors Imputation <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.impute.KNNImputer</span>
│   │   │   │   └── <span class="method-description">✓ Preserves feature relationships, good for mixed data types</span>
│   │   │   └── Matrix Factorization <span class="importance-marker level-3">(3/5)</span>
│   │   │       ├── <span class="package">fancyimpute.MatrixFactorization</span>
│   │   │       ├── <span class="package">fancyimpute.NuclearNormMinimization</span>
│   │   │       └── <span class="method-description">✓ Advanced technique for high-dimensional sparse data</span>
│   │   └── Domain-Specific Imputation <span class="importance-marker level-3">(3/5)</span>
│   │       ├── Time Series Interpolation <span class="importance-marker level-3">(3/5)</span>
│   │       │   ├── <span class="package">pandas.DataFrame.interpolate()</span>
│   │       │   ├── <span class="package">scipy.interpolate.interp1d</span>
│   │       │   └── <span class="method-description">✓ Linear, polynomial, spline interpolation for time series</span>
│   │       └── Seasonal Decomposition <span class="importance-marker level-3">(3/5)</span>
│   │           ├── <span class="package">statsmodels.tsa.seasonal.seasonal_decompose</span>
│   │           └── <span class="method-description">✓ Handles seasonal patterns in time series data</span>
│   │
│   ├── <span class="highlight">1.3 Outlier Detection & Treatment</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── Statistical Methods <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── Z-Score Method <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">scipy.stats.zscore</span>
│   │   │   │   └── <span class="method-description">✓ Identifies outliers beyond 3 standard deviations</span>
│   │   │   ├── Interquartile Range (IQR) <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">pandas.DataFrame.quantile()</span>
│   │   │   │   └── <span class="method-description">✓ Robust to extreme values, uses Q1 and Q3</span>
│   │   │   └── Modified Z-Score <span class="importance-marker level-3">(3/5)</span>
│   │   │       ├── <span class="package">scipy.stats.median_abs_deviation</span>
│   │   │       └── <span class="method-description">✓ Uses median instead of mean, more robust</span>
│   │   └── Machine Learning Methods <span class="importance-marker level-3">(3/5)</span>
│   │       ├── Isolation Forest <span class="importance-marker level-3">(3/5)</span>
│   │       │   ├── <span class="package">sklearn.ensemble.IsolationForest</span>
│   │       │   └── <span class="method-description">✓ Unsupervised anomaly detection, good for high dimensions</span>
│   │       ├── Local Outlier Factor (LOF) <span class="importance-marker level-3">(3/5)</span>
│   │       │   ├── <span class="package">sklearn.neighbors.LocalOutlierFactor</span>
│   │       │   └── <span class="method-description">✓ Density-based outlier detection</span>
│   │       └── One-Class SVM <span class="importance-marker level-2">(2/5)</span>
│   │           ├── <span class="package">sklearn.svm.OneClassSVM</span>
│   │           └── <span class="method-description">✓ Kernel-based outlier detection</span>
│   │
│   ├── <span class="highlight">1.4 Data Integration</span> <span class="importance-marker level-3">(3/5)</span>
│   │   ├── Schema Integration <span class="importance-marker level-3">(3/5)</span>
│   │   │   ├── <span class="package">pandas.DataFrame.merge()</span>
│   │   │   └── <span class="method-description">✓ Combining data from heterogeneous sources</span>
│   │   └── Entity Resolution <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">recordlinkage</span>
│   │       └── <span class="method-description">✓ Identifying and linking records that refer to the same entity</span>
│   │
│   └── <span class="highlight">1.5 Data Transformation</span> <span class="importance-marker level-4">(4/5)</span>
│       ├── Aggregation <span class="importance-marker level-4">(4/5)</span>
│       │   ├── <span class="package">pandas.DataFrame.groupby().agg()</span>
│       │   └── <span class="method-description">✓ Summarizing data (e.g., sum, mean, count)</span>
│       ├── Generalization <span class="importance-marker level-3">(3/5)</span>
│       │   ├── <span class="package">Custom logic</span>
│       │   └── <span class="method-description">✓ Replacing low-level data with high-level concepts (e.g., age ranges)</span>
│       └── Normalization (Data Warehousing Context) <span class="importance-marker level-3">(3/5)</span>
│           ├── <span class="package">SQL techniques</span>
│           └── <span class="method-description">✓ Reducing data redundancy and improving data integrity</span>
│
├── <span class="priority-2">2. FEATURE ENGINEERING (HIGH IMPORTANCE - Second Priority)</span> <span class="importance-marker level-4">(4/5)</span>
│   ├── <span class="highlight">Categorical Encoding</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── One-Hot Encoding <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">sklearn.preprocessing.OneHotEncoder</span>
│   │   │   ├── <span class="package">pandas.get_dummies()</span>
│   │   │   └── <span class="method-description">✓ Best for nominal categories, avoids ordinal assumptions</span>
│   │   ├── Label Encoding <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.preprocessing.LabelEncoder</span>
│   │   │   └── <span class="method-description">✓ Suitable for ordinal categories with natural ordering</span>
│   │   ├── Ordinal Encoding <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.preprocessing.OrdinalEncoder</span>
│   │   │   └── <span class="method-description">✓ Preserves ordinal relationships between categories</span>
│   │   ├── Target Encoding <span class="importance-marker level-3">(3/5)</span>
│   │   │   ├── <span class="package">category_encoders.TargetEncoder</span>
│   │   │   ├── <span class="package">category_encoders.MEstimateEncoder</span>
│   │   │   └── <span class="method-description">✓ Uses target statistics, good for high-cardinality categories</span>
│   │   ├── Binary Encoding <span class="importance-marker level-2">(2/5)</span>
│   │   │   ├── <span class="package">category_encoders.BinaryEncoder</span>
│   │   │   └── <span class="method-description">✓ Reduces dimensionality compared to one-hot</span>
│   │   ├── Hash Encoding <span class="importance-marker level-2">(2/5)</span>
│   │   │   ├── <span class="package">category_encoders.HashingEncoder</span>
│   │   │   └── <span class="method-description">✓ Fixed-size output, handles unseen categories</span>
│   │   └── Frequency Encoding <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">category_encoders.CountEncoder</span>
│   │       └── <span class="method-description">✓ Replaces categories with their occurrence frequency</span>
│   │
│   ├── <span class="highlight">Numerical Transformations</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── Scaling & Normalization <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── StandardScaler (Z-score) <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">sklearn.preprocessing.StandardScaler</span>
│   │   │   │   └── <span class="method-description">✓ Mean=0, Std=1, best for normally distributed data</span>
│   │   │   ├── MinMaxScaler <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">sklearn.preprocessing.MinMaxScaler</span>
│   │   │   │   └── <span class="method-description">✓ Scales to [0,1] range, preserves relationships</span>
│   │   │   ├── RobustScaler <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.preprocessing.RobustScaler</span>
│   │   │   │   └── <span class="method-description">✓ Uses median and IQR, robust to outliers</span>
│   │   │   ├── MaxAbsScaler <span class="importance-marker level-3">(3/5)</span>
│   │   │   │   ├── <span class="package">sklearn.preprocessing.MaxAbsScaler</span>
│   │   │   │   └── <span class="method-description">✓ Scales by maximum absolute value, preserves sparsity</span>
│   │   │   └── Normalizer <span class="importance-marker level-3">(3/5)</span>
│   │   │       ├── <span class="package">sklearn.preprocessing.Normalizer</span>
│   │   │       └── <span class="method-description">✓ Scales individual samples to unit norm</span>
│   │   ├── Distribution Transformation <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── Log Transform <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">numpy.log1p()</span>
│   │   │   │   ├── <span class="package">numpy.log()</span>
│   │   │   │   └── <span class="method-description">✓ Reduces right skewness, handles positive values</span>
│   │   │   ├── Square Root Transform <span class="importance-marker level-3">(3/5)</span>
│   │   │   │   ├── <span class="package">numpy.sqrt()</span>
│   │   │   │   └── <span class="method-description">✓ Moderate skewness reduction</span>
│   │   │   ├── Box-Cox Transform <span class="importance-marker level-3">(3/5)</span>
│   │   │   │   ├── <span class="package">scipy.stats.boxcox()</span>
│   │   │   │   ├── <span class="package">sklearn.preprocessing.PowerTransformer(method='box-cox')</span>
│   │   │   │   └── <span class="method-description">✓ Optimal power transformation for positive data</span>
│   │   │   └── Yeo-Johnson Transform <span class="importance-marker level-3">(3/5)</span>
│   │   │       ├── <span class="package">sklearn.preprocessing.PowerTransformer(method='yeo-johnson')</span>
│   │   │       └── <span class="method-description">✓ Handles both positive and negative values</span>
│   │   └── Binning/Discretization <span class="importance-marker level-4">(4/5)</span>
│   │       ├── Equal-Width Binning <span class="importance-marker level-4">(4/5)</span>
│   │       │   ├── <span class="package">sklearn.preprocessing.KBinsDiscretizer(strategy='uniform')</span>
│   │       │   ├── <span class="package">pandas.cut()</span>
│   │       │   └── <span class="method-description">✓ Equal-sized intervals, may have unequal frequencies</span>
│   │       ├── Equal-Frequency Binning <span class="importance-marker level-4">(4/5)</span>
│   │       │   ├── <span class="package">sklearn.preprocessing.KBinsDiscretizer(strategy='quantile')</span>
│   │       │   ├── <span class="package">pandas.qcut()</span>
│   │       │   └── <span class="method-description">✓ Equal sample sizes per bin</span>
│   │       └── K-Means Binning <span class="importance-marker level-3">(3/5)</span>
│   │           ├── <span class="package">sklearn.preprocessing.KBinsDiscretizer(strategy='kmeans')</span>
│   │           └── <span class="method-description">✓ Clusters data points into bins using K-means</span>
│   │
│   ├── <span class="highlight">Feature Creation</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── Polynomial Features <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.preprocessing.PolynomialFeatures</span>
│   │   │   └── <span class="method-description">✓ Creates polynomial and interaction terms</span>
│   │   ├── Interaction Features <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.preprocessing.PolynomialFeatures(interaction_only=True)</span>
│   │   │   └── <span class="method-description">✓ Only interaction terms, no polynomial terms</span>
│   │   ├── Domain-Specific Features <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── Date/Time Features <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">pandas.dt.year, pandas.dt.month, pandas.dt.dayofweek</span>
│   │   │   │   ├── <span class="package">featuretools.primitives.TimeSeriesFeatures</span>
│   │   │   │   └── <span class="method-description">✓ Extract temporal patterns and cyclical features</span>
│   │   │   ├── Text Features (TF-IDF, N-grams) <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.feature_extraction.text.TfidfVectorizer</span>
│   │   │   │   ├── <span class="package">sklearn.feature_extraction.text.CountVectorizer</span>
│   │   │   │   └── <span class="method-description">✓ Convert text to numerical features</span>
│   │   │   └── Geospatial Features <span class="importance-marker level-3">(3/5)</span>
│   │   │       ├── <span class="package">geopy.distance</span>
│   │   │       └── <span class="method-description">✓ Distance calculations, coordinate transformations</span>
│   │   └── Automated Feature Engineering <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">featuretools.dfs()</span>
│   │       ├── <span class="package">tsfresh.extract_features()</span>
│   │       └── <span class="method-description">✓ Automatically generates features from relational data</span>
│
├── <span class="priority-3">3. FEATURE SELECTION (MEDIUM-HIGH IMPORTANCE - Third Priority)</span> <span class="importance-marker level-3">(3/5)</span>
│   ├── <span class="highlight">Filter Methods (Univariate)</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── Statistical Tests <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── Chi-Square Test <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.feature_selection.chi2</span>
│   │   │   │   ├── <span class="package">sklearn.feature_selection.SelectKBest(chi2)</span>
│   │   │   │   └── <span class="method-description">✓ For categorical features vs categorical target</span>
│   │   │   ├── ANOVA F-Test <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.feature_selection.f_classif</span>
│   │   │   │   ├── <span class="package">sklearn.feature_selection.f_regression</span>
│   │   │   │   └── <span class="method-description">✓ For numerical features vs categorical/numerical target</span>
│   │   │   ├── Mutual Information <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.feature_selection.mutual_info_classif</span>
│   │   │   │   ├── <span class="package">sklearn.feature_selection.mutual_info_regression</span>
│   │   │   │   └── <span class="method-description">✓ Captures non-linear relationships</span>
│   │   │   └── Kendall's Tau <span class="importance-marker level-2">(2/5)</span>
│   │   │       ├── <span class="package">scipy.stats.kendalltau</span>
│   │   │       └── <span class="method-description">✓ Non-parametric correlation measure</span>
│   │   ├── Correlation-Based <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── Pearson Correlation <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">pandas.DataFrame.corr()</span>
│   │   │   │   ├── <span class="package">numpy.corrcoef()</span>
│   │   │   │   ├── <span class="package">scipy.stats.pearsonr()</span>
│   │   │   │   └── <span class="method-description">✓ Linear relationships, normally distributed data</span>
│   │   │   ├── Spearman Correlation <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">scipy.stats.spearmanr()</span>
│   │   │   │   ├── <span class="package">pandas.DataFrame.corr(method='spearman')</span>
│   │   │   │   └── <span class="method-description">✓ Monotonic relationships, rank-based</span>
│   │   │   └── Kendall Correlation <span class="importance-marker level-2">(2/5)</span>
│   │   │       ├── <span class="package">scipy.stats.kendalltau()</span>
│   │   │       └── <span class="method-description">✓ Robust to outliers, small sample sizes</span>
│   │   └── Variance-Based <span class="importance-marker level-4">(4/5)</span>
│   │       ├── Low Variance Filter <span class="importance-marker level-4">(4/5)</span>
│   │       │   ├── <span class="package">sklearn.feature_selection.VarianceThreshold</span>
│   │       │   └── <span class="method-description">✓ Removes features with low variance (near-constant)</span>
│   │       └── High Correlation Filter <span class="importance-marker level-4">(4/5)</span>
│   │           ├── <span class="package">Custom implementation with pandas.DataFrame.corr()</span>
│   │           └── <span class="method-description">✓ Removes highly correlated features (multicollinearity)</span>
│   │
│   ├── <span class="highlight">Wrapper Methods (Model-Based)</span> <span class="importance-marker level-3">(3/5)</span>
│   │   ├── Forward Selection <span class="importance-marker level-3">(3/5)</span>
│   │   │   ├── <span class="package">sklearn.feature_selection.SequentialFeatureSelector(direction='forward')</span>
│   │   │   ├── <span class="package">mlxtend.feature_selection.SequentialFeatureSelector</span>
│   │   │   └── <span class="method-description">✓ Starts empty, adds features iteratively</span>
│   │   ├── Backward Elimination <span class="importance-marker level-3">(3/5)</span>
│   │   │   ├── <span class="package">sklearn.feature_selection.SequentialFeatureSelector(direction='backward')</span>
│   │   │   ├── <span class="package">mlxtend.feature_selection.SequentialFeatureSelector</span>
│   │   │   └── <span class="method-description">✓ Starts with all features, removes iteratively</span>
│   │   ├── Recursive Feature Elimination (RFE) <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.feature_selection.RFE</span>
│   │   │   ├── <span class="package">sklearn.feature_selection.RFECV</span>
│   │   │   └── <span class="method-description">✓ Recursively eliminates least important features</span>
│   │   └── Genetic Algorithms <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">sklearn-genetic-opt.GAFeatureSelectionCV</span>
│   │       ├── <span class="package">DEAP</span>
│   │       └── <span class="method-description">✓ Evolutionary approach to feature selection</span>
│   │
│   ├── <span class="highlight">Embedded Methods (Intrinsic)</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── Tree-Based Importance <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── Random Forest Importance <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">sklearn.ensemble.RandomForestClassifier.feature_importances_</span>
│   │   │   │   ├── <span class="package">sklearn.ensemble.RandomForestRegressor.feature_importances_</span>
│   │   │   │   └── <span class="method-description">✓ Gini/entropy-based importance, handles interactions</span>
│   │   │   ├── Extra Trees Importance <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.ensemble.ExtraTreesClassifier.feature_importances_</span>
│   │   │   │   └── <span class="method-description">✓ More randomized than Random Forest</span>
│   │   │   ├── XGBoost Importance <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">xgboost.XGBClassifier.feature_importances_</span>
│   │   │   │   ├── <span class="package">xgboost.plot_importance()</span>
│   │   │   │   └── <span class="method-description">✓ Gain, weight, cover importance metrics</span>
│   │   │   ├── LightGBM Importance <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">lightgbm.LGBMClassifier.feature_importances_</span>
│   │   │   │   ├── <span class="package">lightgbm.plot_importance()</span>
│   │   │   │   └── <span class="method-description">✓ Split-based importance, fast training</span>
│   │   │   └── CatBoost Importance <span class="importance-marker level-4">(4/5)</span>
│   │   │       ├── <span class="package">catboost.CatBoostClassifier.feature_importances_</span>
│   │   │       └── <span class="method-description">✓ Handles categorical features natively</span>
│   │   └── Regularization-Based <span class="importance-marker level-4">(4/5)</span>
│   │       ├── L1 Regularization (Lasso) <span class="importance-marker level-5">(5/5)</span>
│   │       │   ├── <span class="package">sklearn.linear_model.Lasso</span>
│   │       │   └── <span class="method-description">✓ Drives coefficients to zero, performs feature selection</span>
│   │       ├── L2 Regularization (Ridge) <span class="importance-marker level-4">(4/5)</span>
│   │       │   ├── <span class="package">sklearn.linear_model.Ridge</span>
│   │       │   └── <span class="method-description">✓ Shrinks coefficients, prevents overfitting, no explicit selection</span>
│   │       └── Elastic Net (L1+L2) <span class="importance-marker level-4">(4/5)</span>
│   │           ├── <span class="package">sklearn.linear_model.ElasticNet</span>
│   │           └── <span class="method-description">✓ Combines L1 and L2, robust to correlated features</span>
│   │
│   └── <span class="highlight">Hybrid Methods</span> <span class="importance-marker level-3">(3/5)</span>
│       ├── SelectFromModel <span class="importance-marker level-3">(3/5)</span>
│       │   ├── <span class="package">sklearn.feature_selection.SelectFromModel</span>
│       │   └── <span class="method-description">✓ Uses model's feature importance/coefficients to select features</span>
│       └── Boruta Algorithm <span class="importance-marker level-2">(2/5)</span>
│           ├── <span class="package">boruta_py.BorutaPy</span>
│           └── <span class="method-description">✓ All-relevant feature selection using Random Forest</span>
│
├── <span class="priority-4">4. MODEL SELECTION (HIGH IMPORTANCE - Fourth Priority)</span> <span class="importance-marker level-5">(5/5)</span>
│   ├── <span class="highlight">4.1 Algorithm Selection</span> <span class="importance-marker level-5">(5/5)</span>
│   │   ├── Supervised Learning Algorithms <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── Classification (e.g., Logistic Regression, SVM, Decision Trees, Random Forest, XGBoost)
│   │   │   │   ├── <span class="package">sklearn.linear_model</span>
│   │   │   │   ├── <span class="package">sklearn.svm</span>
│   │   │   │   ├── <span class="package">sklearn.tree</span>
│   │   │   │   ├── <span class="package">sklearn.ensemble</span>
│   │   │   │   ├── <span class="package">xgboost</span>
│   │   │   │   └── <span class="method-description">✓ Choosing the right algorithm based on problem type and data characteristics</span>
│   │   │   └── Regression (e.g., Linear Regression, Ridge, Lasso, SVR, Gradient Boosting)
│   │   │       ├── <span class="package">sklearn.linear_model</span>
│   │   │       ├── <span class="package">sklearn.svm</span>
│   │   │       ├── <span class="package">sklearn.ensemble</span>
│   │   │       └── <span class="method-description">✓ Selecting models for continuous target variables</span>
│   │   └── Unsupervised Learning Algorithms <span class="importance-marker level-4">(4/5)</span>
│   │       ├── Clustering (e.g., K-Means, DBSCAN, Hierarchical Clustering)
│   │       │   ├── <span class="package">sklearn.cluster</span>
│   │       │   └── <span class="method-description">✓ Grouping similar data points</span>
│   │       └── Dimensionality Reduction (e.g., PCA, t-SNE)
│   │           ├── <span class="package">sklearn.decomposition</span>
│   │           ├── <span class="package">sklearn.manifold</span>
│   │           └── <span class="method-description">✓ Reducing feature space for visualization or efficiency</span>
│   │
│   ├── <span class="highlight">4.2 Hyperparameter Tuning</span> <span class="importance-marker level-5">(5/5)</span>
│   │   ├── Grid Search <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">sklearn.model_selection.GridSearchCV</span>
│   │   │   └── <span class="method-description">✓ Exhaustive search over a specified parameter grid</span>
│   │   ├── Random Search <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.model_selection.RandomizedSearchCV</span>
│   │   │   └── <span class="method-description">✓ Random search over parameters from a distribution</span>
│   │   ├── Bayesian Optimization <span class="importance-marker level-3">(3/5)</span>
│   │   │   ├── <span class="package">scikit-optimize</span>
│   │   │   ├── <span class="package">hyperopt</span>
│   │   │   └── <span class="method-description">✓ Uses probabilistic model to find optimal hyperparameters efficiently</span>
│   │   └── Automated ML (AutoML) <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">Auto-Sklearn</span>
│   │       ├── <span class="package">TPOT</span>
│   │       └── <span class="method-description">✓ Automates hyperparameter tuning and model selection</span>
│   │
│   └── <span class="highlight">4.3 Cross-Validation Strategies</span> <span class="importance-marker level-5">(5/5)</span>
│       ├── K-Fold Cross-Validation <span class="importance-marker level-5">(5/5)</span>
│       │   ├── <span class="package">sklearn.model_selection.KFold</span>
│       │   ├── <span class="package">sklearn.model_selection.cross_val_score</span>
│       │   └── <span class="method-description">✓ Standard for robust model evaluation, reduces variance</span>
│       ├── Stratified K-Fold <span class="importance-marker level-5">(5/5)</span>
│       │   ├── <span class="package">sklearn.model_selection.StratifiedKFold</span>
│       │   └── <span class="method-description">✓ Preserves class proportions in each fold, essential for imbalanced data</span>
│       ├── Leave-One-Out Cross-Validation (LOOCV) <span class="importance-marker level-2">(2/5)</span>
│       │   ├── <span class="package">sklearn.model_selection.LeaveOneOut</span>
│       │   └── <span class="method-description">✓ High computational cost, used for small datasets</span>
│       └── Time Series Cross-Validation <span class="importance-marker level-3">(3/5)</span>
│           ├── <span class="package">sklearn.model_selection.TimeSeriesSplit</span>
│           └── <span class="method-description">✓ Preserves temporal order, crucial for time series models</span>
│
├── <span class="priority-5">5. MODEL TRAINING & EVALUATION (CRITICAL - Fifth Priority)</span> <span class="importance-marker level-5">(5/5)</span>
│   ├── <span class="highlight">5.1 Model Training</span> <span class="importance-marker level-5">(5/5)</span>
│   │   ├── Data Splitting (Train/Test/Validation) <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">sklearn.model_selection.train_test_split</span>
│   │   │   └── <span class="method-description">✓ Essential for unbiased evaluation of model performance</span>
│   │   ├── Model Fitting <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">model.fit(X_train, y_train)</span>
│   │   │   └── <span class="method-description">✓ The core process of learning patterns from training data</span>
│   │   └── Ensemble Methods <span class="importance-marker level-4">(4/5)</span>
│   │       ├── Bagging (e.g., Random Forest) <span class="importance-marker level-4">(4/5)</span>
│   │       │   ├── <span class="package">sklearn.ensemble.BaggingClassifier</span>
│   │       │   └── <span class="method-description">✓ Training multiple models independently and averaging predictions</span>
│   │       ├── Boosting (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) <span class="importance-marker level-5">(5/5)</span>
│   │       │   ├── <span class="package">sklearn.ensemble</span>
│   │       │   ├── <span class="package">xgboost</span>
│   │       │   ├── <span class="package">lightgbm</span>
│   │       │   ├── <span class="package">catboost</span>
│   │       │   └── <span class="method-description">✓ Sequentially building models to correct errors of previous models</span>
│   │       └── Stacking <span class="importance-marker level-3">(3/5)</span>
│   │           ├── <span class="package">sklearn.ensemble.StackingClassifier</span>
│   │           └── <span class="method-description">✓ Training a meta-model on predictions of multiple base models</span>
│   │
│   ├── <span class="highlight">5.2 Model Evaluation (Metrics)</span> <span class="importance-marker level-5">(5/5)</span>
│   │   ├── Classification Metrics <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── Accuracy <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── Precision, Recall, F1-Score <span class="importance-marker level-5">(5/5)</span>
│   │   │   │   ├── <span class="package">sklearn.metrics.accuracy_score</span>
│   │   │   │   ├── <span class="package">sklearn.metrics.precision_score</span>
│   │   │   │   ├── <span class="package">sklearn.metrics.recall_score</span>
│   │   │   │   ├── <span class="package">sklearn.metrics.f1_score</span>
│   │   │   │   └── <span class="method-description">✓ For evaluating classification model performance</span>
│   │   │   ├── ROC AUC <span class="importance-marker level-4">(4/5)</span>
│   │   │   │   ├── <span class="package">sklearn.metrics.roc_auc_score</span>
│   │   │   │   └── <span class="method-description">✓ Measures classifier's ability to distinguish between classes</span>
│   │   │   └── Confusion Matrix <span class="importance-marker level-5">(5/5)</span>
│   │   │       ├── <span class="package">sklearn.metrics.confusion_matrix</span>
│   │   │       └── <span class="method-description">✓ Visualizes performance of a classification model</span>
│   │   └── Regression Metrics <span class="importance-marker level-5">(5/5)</span>
│   │       ├── Mean Squared Error (MSE), Root Mean Squared Error (RMSE) <span class="importance-marker level-5">(5/5)</span>
│   │       │   ├── <span class="package">sklearn.metrics.mean_squared_error</span>
│   │       │   └── <span class="method-description">✓ Common metrics for regression, penalize larger errors more</span>
│   │       ├── Mean Absolute Error (MAE) <span class="importance-marker level-4">(4/5)</span>
│   │       │   ├── <span class="package">sklearn.metrics.mean_absolute_error</span>
│   │       │   └── <span class="method-description">✓ Less sensitive to outliers than MSE</span>
│   │       └── R-squared ($R^2$) <span class="importance-marker level-4">(4/5)</span>
│   │           ├── <span class="package">sklearn.metrics.r2_score</span>
│   │           └── <span class="method-description">✓ Proportion of variance in dependent variable predictable from independent variables</span>
│   │
│   └── <span class="highlight">5.3 Overfitting/Underfitting Diagnosis</span> <span class="importance-marker level-5">(5/5)</span>
│       ├── Learning Curves <span class="importance-marker level-4">(4/5)</span>
│       │   ├── <span class="package">sklearn.model_selection.learning_curve</span>
│       │   └── <span class="method-description">✓ Visualizing model performance with increasing training data size</span>
│       ├── Validation Curves <span class="importance-marker level-4">(4/5)</span>
│       │   ├── <span class="package">sklearn.model_selection.validation_curve</span>
│       │   └── <span class="method-description">✓ Visualizing model performance with varying hyperparameter values</span>
│       └── Bias-Variance Trade-off <span class="importance-marker level-5">(5/5)</span>
│           ├── <span class="package">Conceptual understanding</span>
│           └── <span class="method-description">✓ Balancing model complexity to minimize generalization error</span>
│
├── <span class="priority-4">6. REGULARIZATION STRATEGIES (MEDIUM IMPORTANCE - Sixth Priority)</span> <span class="importance-marker level-4">(4/5)</span>
│   ├── <span class="highlight">Linear Model Regularization</span> <span class="importance-marker level-4">(4/5)</span>
│   │   ├── L1 Regularization (Lasso) <span class="importance-marker level-5">(5/5)</span>
│   │   │   ├── <span class="package">sklearn.linear_model.Lasso</span>
│   │   │   └── <span class="method-description">✓ Feature selection property, drives weights to zero</span>
│   │   ├── L2 Regularization (Ridge) <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.linear_model.Ridge</span>
│   │   │   └── <span class="method-description">✓ Shrinks coefficients, prevents overfitting, no explicit selection</span>
│   │   ├── Elastic Net (L1+L2) <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.linear_model.ElasticNet</span>
│   │   │   └── <span class="method-description">✓ Hybrid of L1 and L2, handles correlated features well</span>
│   │   └── Group Lasso <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">sklearn_learn_contrib.group_lasso</span>
│   │       └── <span class="method-description">✓ Selects/eliminates groups of features together</span>
│   │
│   ├── <span class="highlight">Tree-Based Regularization</span> <span class="importance-marker level-3">(3/5)</span>
│   │   ├── Max Depth Control <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.tree.DecisionTreeClassifier(max_depth)</span>
│   │   │   ├── <span class="package">xgboost.XGBClassifier(max_depth)</span>
│   │   │   └── <span class="method-description">✓ Limits tree growth, prevents overfitting</span>
│   │   ├── Min Samples Split/Leaf <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.tree.DecisionTreeClassifier(min_samples_split)</span>
│   │   │   ├── <span class="package">sklearn.tree.DecisionTreeClassifier(min_samples_leaf)</span>
│   │   │   └── <span class="method-description">✓ Controls minimum samples required to split/form a leaf</span>
│   │   └── Feature Subsetting (e.g., Random Forest) <span class="importance-marker level-3">(3/5)</span>
│   │       ├── <span class="package">sklearn.ensemble.RandomForestClassifier(max_features)</span>
│   │       └── <span class="method-description">✓ Randomly selects a subset of features for each tree</span>
│   │
│   └── <span class="highlight">Neural Network Regularization</span> <span class="importance-marker level-3">(3/5)</span>
│       ├── Dropout <span class="importance-marker level-4">(4/5)</span>
│       │   ├── <span class="package">tensorflow.keras.layers.Dropout</span>
│       │   ├── <span class="package">torch.nn.Dropout</span>
│       │   └── <span class="method-description">✓ Randomly sets a fraction of input units to 0 at each update</span>
│       ├── Batch Normalization <span class="importance-marker level-4">(4/5)</span>
│       │   ├── <span class="package">tensorflow.keras.layers.BatchNormalization</span>
│   │   │   └── <span class="method-description">✓ Normalizes layer inputs, reduces internal covariate shift</span>
│   │   └── Weight Decay (L1/L2 penalty) <span class="importance-marker level-3">(3/5)</span>
│       │   ├── <span class="package">tensorflow.keras.regularizers.l1_l2</span>
│       │   ├── <span class="package">torch.optim.Adam(weight_decay)</span>
│       │   └── <span class="method-description">✓ Adds penalty to weights, similar to L1/L2 regularization</span>
│
└── <span class="priority-5">7. DIMENSIONALITY REDUCTION (CONTEXT-DEPENDENT - Seventh Priority)</span> <span class="importance-marker level-1">(1/5)</span>
    ├── <span class="highlight">Linear Methods</span> <span class="importance-marker level-3">(3/5)</span>
    │   ├── Principal Component Analysis (PCA) <span class="importance-marker level-4">(4/5)</span>
    │   │   ├── <span class="package">sklearn.decomposition.PCA</span>
    │   │   └── <span class="method-description">✓ Transforms data to orthogonal components, retains variance</span>
    │   ├── Linear Discriminant Analysis (LDA) <span class="importance-marker level-3">(3/5)</span>
    │   │   ├── <span class="package">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</span>
│   │   │   └── <span class="method-description">✓ Maximizes class separability, supervised</span>
│   │   ├── Independent Component Analysis (ICA) <span class="importance-marker level-2">(2/5)</span>
│   │   │   ├── <span class="package">sklearn.decomposition.FastICA</span>
│   │   │   └── <span class="method-description">✓ Separates multivariate signal into independent components</span>
│   │   └── Factor Analysis <span class="importance-marker level-2">(2/5)</span>
│   │       ├── <span class="package">sklearn.decomposition.FactorAnalysis</span>
│   │       └── <span class="method-description">✓ Explains variance using a smaller number of latent factors</span>
│   │
│   ├── <span class="highlight">Non-Linear Methods</span> <span class="importance-marker level-3">(3/5)</span>
│   │   ├── t-Distributed Stochastic Neighbor Embedding (t-SNE) <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">sklearn.manifold.TSNE</span>
│   │   │   └── <span class="method-description">✓ Best for visualization, preserves local structure</span>
│   │   ├── UMAP (Uniform Manifold Approximation and Projection) <span class="importance-marker level-4">(4/5)</span>
│   │   │   ├── <span class="package">umap-learn.UMAP</span>
│   │   │   └── <span class="method-description">✓ Faster than t-SNE, good for visualization and general embedding</span>
│   │   ├── Kernel PCA <span class="importance-marker level-2">(2/5)</span>
│   │   │   ├── <span class="package">sklearn.decomposition.KernelPCA</span>
│   │   │   └── <span class="method-description">✓ Non-linear PCA using kernel tricks</span>
│   │   └── Autoencoders <span class="importance-marker level-3">(3/5)</span>
│   │       ├── <span class="package">tensorflow.keras.models.Sequential</span>
│   │       ├── <span class="package">torch.nn.Module</span>
│   │       └── <span class="method-description">✓ Neural network for learning compressed data representation</span>
│   │
│   └── <span class="highlight">Sparse Methods</span> <span class="importance-marker level-2">(2/5)</span>
│       ├── Sparse PCA <span class="importance-marker level-2">(2/5)</span>
│       │   ├── <span class="package">sklearn.decomposition.SparsePCA</span>
│       │   └── <span class="method-description">✓ PCA with sparse components, improves interpretability</span>
│       └── Dictionary Learning <span class="importance-marker level-2">(2/5)</span>
│           ├── <span class="package">sklearn.decomposition.DictionaryLearning</span>
│           └── <span class="method-description">✓ Learns a dictionary of sparse components</span>
            </div>
        </div>

        <div class="section-header">
            ✅ Order of Usage & Importance (Generalized ML Pipeline)
        </div>

        <div class="pipeline-order">
            <div class="phase-card phase-1">
                <h3>Phase 1: Data Preprocessing & Cleaning (Critical)</h3>
                <ul>
                    <li>Data Cleaning (handling inconsistencies, duplicates, noise)</li>
                    <li>Missing Value Imputation - Must be done first</li>
                    <li>Outlier Detection & Treatment - Early identification crucial</li>
                    <li>Data Integration & Transformation (aggregation, generalization)</li>
                </ul>
            </div>
            <div class="phase-card phase-2">
                <h3>Phase 2: Feature Engineering (High Importance)</h3>
                <ul>
                    <li>Categorical Encoding - Transform categorical variables</li>
                    <li>Numerical Scaling - Normalize/standardize features</li>
                    <li>Feature Creation - Generate new meaningful features</li>
                </ul>
            </div>
            <div class="phase-card phase-3">
                <h3>Phase 3: Feature Selection (Medium-High Importance)</h3>
                <ul>
                    <li>Filter Methods - Quick elimination of irrelevant features</li>
                    <li>Wrapper/Embedded Methods - Model-based selection</li>
                    <li>Correlation Analysis - Remove redundant features</li>
                </ul>
            </div>
            <div class="phase-card phase-4">
                <h3>Phase 4: Model Selection (High Importance)</h3>
                <ul>
                    <li>Algorithm Selection - Choosing the right model type</li>
                    <li>Hyperparameter Tuning - Optimizing model parameters</li>
                    <li>Cross-Validation Strategies - Robust evaluation setup</li>
                </ul>
            </div>
            <div class="phase-card phase-5">
                <h3>Phase 5: Model Training & Evaluation (Critical)</h3>
                <ul>
                    <li>Data Splitting - Preparing data for training and testing</li>
                    <li>Model Fitting - Training the chosen model</li>
                    <li>Ensemble Methods - Combining models for better performance</li>
                    <li>Model Evaluation - Using metrics to assess performance</li>
                    <li>Overfitting/Underfitting Diagnosis - Identifying model issues</li>
                </ul>
            </div>
            <div class="phase-card phase-6">
                <h3>Phase 6: Regularization Strategies (Medium Importance)</h3>
                <ul>
                    <li>Applied during model training - Prevent overfitting</li>
                    <li>Hyperparameter tuning - Optimize regularization strength</li>
                </ul>
            </div>
            <div class="phase-card phase-7">
                <h3>Phase 7: Dimensionality Reduction (Context-Dependent)</h3>
                <ul>
                    <li>Used when necessary - High-dimensional data, visualization</li>
                    <li>Computational efficiency - Reduce training time</li>
                </ul>
            </div>
        </div>

        <div class="section-header">
            📦 Key Python Packages Summary
        </div>

        <table class="package-table">
            <thead>
                <tr>
                    <th>Category</th>
                    <th>Primary Packages</th>
                    <th>Specialized Packages</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>General ML</td>
                    <td><code>scikit-learn</code>, <code>pandas</code>, <code>numpy</code></td>
                    <td><code>scipy</code>, <code>statsmodels</code></td>
                </tr>
                <tr>
                    <td>Data Cleaning & Integration</td>
                    <td><code>pandas</code></td>
                    <td><code>recordlinkage</code></td>
                </tr>
                <tr>
                    <td>Feature Engineering</td>
                    <td><code>sklearn.preprocessing</code>, <code>category_encoders</code></td>
                    <td><code>featuretools</code>, <code>tsfresh</code></td>
                </tr>
                <tr>
                    <td>Feature Selection</td>
                    <td><code>sklearn.feature_selection</code></td>
                    <td><code>mlxtend</code>, <code>boruta</code>, <code>sklearn-genetic</code></td>
                </tr>
                <tr>
                    <td>Model Selection & Training</td>
                    <td><code>scikit-learn</code>, <code>xgboost</code>, <code>lightgbm</code></td>
                    <td><code>catboost</code>, <code>scikit-optimize</code>, <code>hyperopt</code>, <code>Auto-Sklearn</code>, <code>TPOT</code></td>
                </tr>
                <tr>
                    <td>Deep Learning</td>
                    <td><code>tensorflow</code>, <code>pytorch</code></td>
                    <td><code>keras</code></td>
                </tr>
                <tr>
                    <td>Dimensionality Reduction</td>
                    <td><code>sklearn.decomposition</code>, <code>sklearn.manifold</code></td>
                    <td><code>umap-learn</code></td>
                </tr>
                <tr>
                    <td>Model Evaluation</td>
                    <td><code>sklearn.metrics</code></td>
                    <td><code>yellowbrick</code></td>
                </tr>
            </tbody>
        </table>

        <div class="section-header">
            ✨ Best Practices Order
        </div>

        <div class="best-practices">
            <h3>Recommended Workflow for Machine Learning Pipeline</h3>
            <ol>
                <li>Always start with **Data Preprocessing & Cleaning**: Handle missing values, outliers, inconsistencies, and integrate/transform data.</li>
                <li>Proceed with **Feature Engineering**: Create new features and transform existing ones to enhance model performance.</li>
                <li>Apply **Feature Selection**: Filter out irrelevant or redundant features to improve efficiency and interpretability.</li>
                <li>Move to **Model Selection**: Choose appropriate algorithms, tune hyperparameters, and set up robust cross-validation.</li>
                <li>Perform **Model Training & Evaluation**: Split data, fit models, use ensemble techniques, evaluate with relevant metrics, and diagnose overfitting/underfitting.</li>
                <li>Integrate **Regularization Strategies** during model training to prevent overfitting.</li>
                <li>Consider **Dimensionality Reduction** for high-dimensional datasets, especially for visualization or computational efficiency.</li>
                <li>Continuously **Iterate and Refine**: The ML pipeline is iterative; revisit earlier steps based on model performance.</li>
            </ol>
        </div>
    </div>
</body>
</html>
